# -*- coding: utf-8 -*-
"""Nellie_Dweben_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SrC_txHc-iimYLpoHCyi3GDjpAs40Nvd
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
fifa = pd.read_csv(r'C:\Users\epheh\Downloads\male_players (legacy).csv',na_values='.')
fifa

fifa.shape

fifa.info

L = []
L_less =[]
for i in fifa.columns:
    if((fifa[i].isnull().sum())<(0.2 *(fifa.shape[0]))):
        L.append(i)
    else:
        L_less.append(i)
fifa=fifa[L]

L

L_less

# separate the numeric data from the non-numeric data
numeric_data = fifa.select_dtypes(include = np.number)
non_numeric = fifa.select_dtypes(include = ['object'])

#non_numeric

def drop_columns_with_high_nulls(df, threshold=0.3):
    # Calculate the number of rows in the DataFrame
    num_rows = len(df)

    # Iterate through each column and check the percentage of null values
    columns_to_drop = [column for column in df.columns if df[column].isnull().sum() / num_rows >= threshold]

    # Drop the columns
    df_cleaned = df.drop(columns=columns_to_drop)

    return df_cleaned



numeric_data = pd.read_csv(r'C:\Users\epheh\Downloads\male_players (legacy).csv')
numeric_data_cleaned = drop_columns_with_high_nulls(numeric_data)
print(numeric_data_cleaned)

numeric_data_cleaned



# def convert_all_columns_to_numeric(df):
#     # Iterate through each column index
#     for col_index in range(df.shape[1]):
#         # Check if the column data type is object (string)
#         if df.iloc[:, col_index].dtype == 'object':
#             # Convert the column to numeric, coerce errors to NaN
#             df.iloc[:, col_index] = pd.to_numeric(df.iloc[:, col_index], errors='coerce')

#     return df

# # Example usage
# # Load your dataset


# # Convert all string columns to numeric
# df = convert_all_columns_to_numeric(numeric_data_cleaned)

# # Display the DataFrame to check the changes
# print(df)

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
imp = IterativeImputer(max_iter=10,random_state =0)
numeric_data_cleaned= pd.DataFrame(np.round(imp.fit_transform(numeric_data_cleaned)),columns=numeric_data_cleaned.columns)
numeric_data_cleaned

numeric_data_cleaned['fifa'].isnull().sum()

# Clean and impute the numeric data
numeric_data_cleaned = clean_and_impute(numeric_data)

# Display the cleaned and imputed DataFrame
print(numeric_data_cleaned)

# Calculate the correlation matrix
correlation_matrix = numeric_data_cleaned.corr()

# Display the correlation with the dependent variable
# Replace 'dependent_variable' with your actual dependent variable name
correlation_with_target = correlation_matrix['overall'].drop('overall')

print(correlation_with_target)

y=correlation_matrix['overall']
y

N = 5  # Adjust N to get the number of features you want in your subset
top_features = correlation_with_target.abs().sort_values(ascending=False).head(N).index

print("Top features:\n", top_features)





import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Load the dataset with low_memory=False to handle mixed types
file_path = r'C:\Users\epheh\Downloads\male_players (legacy).csv'
df = pd.read_csv(file_path, low_memory=False)

# Display the first few rows of the dataset
print(df.head())

# Convert all non-numeric columns to numeric where possible, invalid parsing will be set as NaN
df = df.apply(pd.to_numeric, errors='coerce')

# Drop columns with too many missing values (optional, you can adjust the threshold)
df = df.dropna(axis=1, thresh=int(0.5*len(df)))

# Fill remaining missing values with the median of each column
df = df.fillna(df.median())

# Analyze correlations
correlation_matrix = df.corr()

# Display the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# Select the top 5 features with the highest absolute correlation with the target variable 'overall'
correlated_features = correlation_matrix['overall'].abs().sort_values(ascending=False).index[1:6].tolist()

# Create a new DataFrame with the selected features
X = df[correlated_features]
y = df['overall']

# Scale the independent variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert the scaled features back to a DataFrame for easier manipulation
X_scaled_df = pd.DataFrame(X_scaled, columns=correlated_features)

# Display the first few rows of the scaled features
print(X_scaled_df.head())



pip install xgboost

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the dataset with low_memory=False to handle mixed types
file_path = r'C:\Users\epheh\Downloads\male_players (legacy).csv'
df = pd.read_csv(file_path, low_memory=False)

# Display the first few rows of the dataset
print(df.head())

# Keep only the specified features and the target variable
features = ['movement_reactions', 'potential', 'passing', 'wage_eur', 'mentality_composure']
target = 'overall'

df = df[features + [target]]

# Convert all non-numeric columns to numeric where possible, invalid parsing will be set as NaN
df = df.apply(pd.to_numeric, errors='coerce')

# Fill remaining missing values with the median of each column
df = df.fillna(df.median())

# Create the feature matrix (X) and target vector (y)
X = df[features]
y = df[target]

# Scale the independent variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)

# Define the models
models = {
    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=0),
    'XGBoost': XGBRegressor(n_estimators=100, random_state=0),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=0)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')
    print(f"{model_name} Cross-validation MAE: ", -cv_scores.mean())

    # Fit the model
    model.fit(X_train, y_train)

    # Make predictions on the test set
    predictions = model.predict(X_test)

    # Evaluate the model on the test set
    mae = mean_absolute_error(y_test, predictions)
    mse = mean_squared_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    print(f"{model_name} R^2 Score: ", r2)

"""Number 4"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# Load the dataset with low_memory=False to handle mixed types
file_path = r'C:\Users\epheh\Downloads\male_players (legacy).csv'
df = pd.read_csv(file_path, low_memory=False)

# Keep only the specified features and the target variable
features = ['movement_reactions', 'potential', 'passing', 'wage_eur', 'mentality_composure']
target = 'overall'
df = df[features + [target]]

# Convert all non-numeric columns to numeric where possible, invalid parsing will be set as NaN
df = df.apply(pd.to_numeric, errors='coerce')

# Fill remaining missing values with the median of each column
df = df.fillna(df.median())

# Create the feature matrix (X) and target vector (y)
X = df[features]
y = df[target]

# Scale the independent variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)

# Define the models
models = {
    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=0),
    'XGBoost': XGBRegressor(n_estimators=100, random_state=0),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=0)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Cross-validation MAE
    cv_scores_mae = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')
    print(f"{model_name} Cross-validation MAE: ", cv_scores_mae.mean())

    # Fit the model
    model.fit(X_train, y_train)

    # Make predictions on the test set
    predictions = model.predict(X_test)

    # Evaluate the model on the test set
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    print(f"{model_name} MAE: ", mae)
    print(f"{model_name} R^2 Score: ", r2)

    # Fine-tuning with GridSearchCV
    if model_name == 'RandomForest':
        param_grid = {
            'n_estimators': [25, 50, 100],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10]
        }
    elif model_name == 'XGBoost':
        param_grid = {
            'n_estimators': [25, 50, 100],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.3]
        }
    elif model_name == 'GradientBoosting':
        param_grid = {
            'n_estimators': [25, 50, 100],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.3]
        }

    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error', verbose=1)
    grid_search.fit(X_train, y_train)

    # Print the best parameters found
    print(f"Best parameters for {model_name}: ", grid_search.best_params_)

    # Evaluate the best model on the test set
    best_model = grid_search.best_estimator_
    best_predictions = best_model.predict(X_test)
    best_mae = mean_absolute_error(y_test, best_predictions)
    best_r2 = r2_score(y_test, best_predictions)
    print(f"Best {model_name} MAE: ", best_mae)
    print(f"Best {model_name} R^2 Score: ", best_r2)



"""After evaluating the models and fine-tuning their hyperparameters, I found the following R^2 scores for each model. The RandomForest model initially had an R^2 score of 0.8954, which improved to 0.9014 after tuning. The XGBoost model started with an R^2 score of 0.9014 and improved slightly to 0.9020. Lastly, the GradientBoosting model had an initial R^2 score of 0.8869, which increased to 0.9019 after tuning. Therefore, XGBoost achieved the highest R^2 score of 0.9020, making it the best performing model among the three.

Question 5


Use the data from another season(players_22) which was not used during the training to test how good is the model. [5]
"""





import pandas as pd
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# Load the trained model from a pickle file
with open('best_model_xgboost.pkl', 'rb') as file:
    best_model = pickle.load(file)

# Create a pipeline including the scaler and the loaded model
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', best_model)
])

# Load the new dataset (players_22)
file_path_new = r'C:\Users\epheh\Downloads\players_22.csv'
df_new = pd.read_csv(file_path_new, low_memory=False)

# Keep only the specified features and the target variable
features = ['movement_reactions', 'potential', 'passing', 'wage_eur', 'mentality_composure']
target = 'overall'
df_new = df_new[features + [target]]

# Convert all non-numeric columns to numeric where possible, invalid parsing will be set as NaN
df_new = df_new.apply(pd.to_numeric, errors='coerce')

# Fill remaining missing values with the median of each column
df_new = df_new.fillna(df_new.median())

# Create the feature matrix (X) and target vector (y)
X_new = df_new[features]
y_new = df_new[target]

# Use the pipeline to make predictions on the new dataset
predictions_new = pipeline.predict(X_new)

# Evaluate the model on the new dataset
r2_new = r2_score(y_new, predictions_new)

# Output the R^2 score
print(f"New Data R^2 Score: {r2_new}")

import pickle
with open('best_model_xgboost.pkl', 'wb') as file:
    pickle.dump(best_model, file)

